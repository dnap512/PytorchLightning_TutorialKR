{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d4e331",
   "metadata": {},
   "source": [
    "# LIGHTNINGMODULE\n",
    "### 본 문서는 PyTorch Lightning의 [공식 가이드](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#minimal-example)의 한글 번역본입니다. (옮긴이 [dnap512](https://github.com/dnap512), 21.7.14)\n",
    "\n",
    "`LightningModule`은 5가지 부분의 PyTorch 코드로 구성됩니다.\n",
    "\n",
    "- Computations (init)\n",
    "- Train loop (training_step)\n",
    "- Validation loop (validation_step)\n",
    "- Test loop (test_step)\n",
    "- Optimizers (configure_optimizers)\n",
    "\n",
    "\n",
    "[가이드 동영상](https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/pl_mod_vid.m4v)을 참고하세요!\n",
    "\n",
    "몇 가지 사항에 유의하세요.\n",
    "\n",
    "1. 동일한 코드입니다.\n",
    "2. PyTorch 코드는 추상화되지 않고 구성됩니다.\n",
    "3. LightningModule에 없는 다른 모든 코드는 트레이너에 의해 자동화되었습니다.\n",
    "\n",
    "```python\n",
    "net = Net()\n",
    "trainer = Trainer()\n",
    "trainer.fit(net)\n",
    "```\n",
    "4. .cuda()나 .to() 호출은 없습니다. Lightning이 알아서 해줍니다!\n",
    "\n",
    "```python\n",
    "# don't do in lightning\n",
    "x = torch.Tensor(2, 3)\n",
    "x = x.cuda()\n",
    "x = x.to(device)\n",
    "\n",
    "# do this instead\n",
    "x = x  # leave it alone!\n",
    "\n",
    "# or to init a new tensor\n",
    "new_x = torch.Tensor(2, 3)\n",
    "new_x = new_x.type_as(x)\n",
    "```\n",
    "\n",
    "5. Lightning은 여러분을 위해 기본값으로 distrbuted sampler를 다룹니다.\n",
    "\n",
    "```python\n",
    "# Don't do in Lightning...\n",
    "data = MNIST(...)\n",
    "sampler = DistributedSampler(data)\n",
    "DataLoader(data, sampler=sampler)\n",
    "\n",
    "# do this instead\n",
    "data = MNIST(...)\n",
    "DataLoader(data)\n",
    "```\n",
    "\n",
    "6. `LightningModule`은 [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) 이긴 하지만 기능이 추가된 것입니다. 그대로 사용하세요!\n",
    "\n",
    "따라서 Lightning을 사용하려면 약 30분 정도 소요되는 코드를 구성하기만 하면 됩니다(진짜로요).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f5fc9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Minimal Example (최소 예제?)\n",
    "\n",
    "다음은 필요한 최소한의 메서드들만 구현된 예제입니다.\n",
    "\n",
    "\n",
    "다음을 수행하여 훈련할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ddf4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-14T06:54:48.170681Z",
     "start_time": "2021-07-14T06:54:39.352520Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(28 * 28, 10)\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.l1(x.view(x.size(0), -1)))\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)\n",
    "    \n",
    "train_loader = DataLoader(MNIST(os.getcwd(), download=True, transform=transforms.ToTensor()))\n",
    "trainer = pl.Trainer()\n",
    "model = LitModel()\n",
    "\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba68744",
   "metadata": {
    "hidden": true
   },
   "source": [
    "LightningModule에는 많은 편리한 방법이 있지만 알아야 할 핵심 방법은 다음과 같습니다.\n",
    "\n",
    "|이름|설명|\n",
    "|:---|:---|\n",
    "|init|여기서 연산을 정의합니다|\n",
    "|forward|추론만을 위해 작성합니다.(training_step과는 별도로 분리)|\n",
    "|training_step|전체 학습 루프|\n",
    "|validation_step|전체 검증 루프|\n",
    "|test_step|전체 테스트 루프|\n",
    "|configure_optimizers|옵티마이저와 LR scheduler들을 정의합니다|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf95eef",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Training loop\n",
    "\n",
    "학습 루프를 추가하기 위해서 `training_step` method를 추가하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cacb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitClassifier(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ed635",
   "metadata": {},
   "source": [
    "내부에서 Lightning은 다음을 수행합니다(pseudocode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c1a609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put model in train mode\n",
    "model.train()\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "losses = []\n",
    "for batch in train_dataloader:\n",
    "    # forward\n",
    "    loss = training_step(batch)\n",
    "    losses.append(loss.detach())\n",
    "\n",
    "    # clear gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87940a30",
   "metadata": {},
   "source": [
    "#### Training epoch-level metrics\n",
    "\n",
    "만약 여러분이 epoch 단위로 meric을 계산하고 로깅하고 싶다면, `.log()` 메소드를 사용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f0280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y_hat = self.model(x)\n",
    "    loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "    # logs metrics for each training_step,\n",
    "    # and the average across the epoch, to the progress bar and logger\n",
    "    self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f823ad5",
   "metadata": {},
   "source": [
    "`.log` 오브젝트는 전체 Epochs에서 요청된 Metric을 자동으로 Reduce합니다. 내부에서 수행하는 작업의 Psuedocode는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e1bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "for batch in train_dataloader:\n",
    "    # forward\n",
    "    out = training_step(val_batch)\n",
    "\n",
    "    # clear gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "epoch_metric = torch.mean(torch.stack([x['train_loss'] for x in outs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c59f4",
   "metadata": {},
   "source": [
    "#### Train epoch-level operations\n",
    "\n",
    "각 training_step의 모든 출력으로 작업을 수행해야 하는 경우 training_epoch_end를 직접 재정의하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b9ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y_hat = self.model(x)\n",
    "    loss = F.cross_entropy(y_hat, y)\n",
    "    preds = ...\n",
    "    return {'loss': loss, 'other_stuff': preds}\n",
    "\n",
    "def training_epoch_end(self, training_step_outputs):\n",
    "    for pred in training_step_outputs:\n",
    "       # do something"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc559f8",
   "metadata": {},
   "source": [
    "해당하는 Psuedocode는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "for batch in train_dataloader:\n",
    "    # forward\n",
    "    out = training_step(val_batch)\n",
    "\n",
    "    # clear gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "training_epoch_end(outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e6665",
   "metadata": {},
   "source": [
    "#### Training with DataParallel\n",
    "\n",
    "GPU에서 각 배치의 데이터를 분할하는 `accelerator`를 사용하여 훈련할 때, 처리를 위해 마스터 GPU에서 데이터를 집계해야 할 수도 있습니다(dp 또는 ddp2).\n",
    "\n",
    "이 경우 `training_step_end` 메소드를 구현하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y_hat = self.model(x)\n",
    "    loss = F.cross_entropy(y_hat, y)\n",
    "    pred = ...\n",
    "    return {'loss': loss, 'pred': pred}\n",
    "\n",
    "def training_step_end(self, batch_parts):\n",
    "    # predictions from each GPU\n",
    "    predictions = batch_parts['pred']\n",
    "    # losses from each GPU\n",
    "    losses = batch_parts['loss']\n",
    "\n",
    "    gpu_0_prediction = predictions[0]\n",
    "    gpu_1_prediction = predictions[1]\n",
    "\n",
    "    # do something with both outputs\n",
    "    return (losses[0] + losses[1]) / 2\n",
    "\n",
    "def training_epoch_end(self, training_step_outputs):\n",
    "    for out in training_step_outputs:\n",
    "       # do something with preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a9ab0b",
   "metadata": {},
   "source": [
    "Lightning 내부에서 수행되는 전체 Psuedocode는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1479bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "for train_batch in train_dataloader:\n",
    "    batches = split_batch(train_batch)\n",
    "    dp_outs = []\n",
    "    for sub_batch in batches:\n",
    "        # 1\n",
    "        dp_out = training_step(sub_batch)\n",
    "        dp_outs.append(dp_out)\n",
    "\n",
    "    # 2\n",
    "    out = training_step_end(dp_outs)\n",
    "    outs.append(out)\n",
    "\n",
    "# do something with the outputs for all batches\n",
    "# 3\n",
    "training_epoch_end(outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c24116",
   "metadata": {},
   "source": [
    "Validation은 Training과 기본 구조가 같기 때문에 생략합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271248b2",
   "metadata": {},
   "source": [
    "### Test loop\n",
    "\n",
    "테스트 루프를 추가하는 과정은 검증 루프를 추가하는 과정과 동일합니다. 자세한 내용은 위의 섹션을 참조하세요.\n",
    "\n",
    "유일한 차이점은 테스트 루프는 `.test()`가 직접 사용될 때만 호출된다는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b46241",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "trainer = Trainer()\n",
    "trainer.fit()\n",
    "\n",
    "# automatically loads the best weights for you\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06998dc0",
   "metadata": {},
   "source": [
    "`test()`를 호출하는 두 가지 방법이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6f4880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call after training\n",
    "trainer = Trainer()\n",
    "trainer.fit(model)\n",
    "\n",
    "# automatically auto-loads the best weights\n",
    "trainer.test(dataloaders=test_dataloader)\n",
    "\n",
    "# or call with pretrained model\n",
    "model = MyLightningModule.load_from_checkpoint(PATH)\n",
    "trainer = Trainer()\n",
    "trainer.test(model, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962793c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb5039",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "연구 목적으로 LightningModule은 시스템으로서 가장 잘 구성될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c8cd76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-14T07:36:51.543002Z",
     "start_time": "2021-07-14T07:36:48.859256Z"
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Autoencoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(28 * 28, 256), nn.ReLU(), nn.Linear(256, latent_dim))\n",
    "        self.decoder = nn.Sequential(nn.Linear(latent_dim, 256), nn.ReLU(), nn.Linear(256, 28 * 28))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        # encode\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        # decode\n",
    "        recons = self.decoder(z)\n",
    "\n",
    "        # reconstruction\n",
    "        reconstruction_loss = nn.functional.mse_loss(recons, x)\n",
    "        return reconstruction_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        recons = self.decoder(z)\n",
    "        reconstruction_loss = nn.functional.mse_loss(recons, x)\n",
    "        self.log('val_reconstruction', reconstruction_loss)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        # encode\n",
    "        # for predictions, we could return the embedding or the reconstruction or both based on our need.\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f7b360",
   "metadata": {},
   "source": [
    "그리고 아래처럼 모델을 훈련할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5564c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder()\n",
    "trainer = pl.Trainer(gpus=1)\n",
    "trainer.fit(autoencoder, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b831705",
   "metadata": {},
   "source": [
    "이 간단한 모델은 다음과 같은 예제를 생성합니다(인코더와 디코더가 너무 약하네요).\n",
    "\n",
    "![이미지](https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/pl_docs/ae_docs.png)\n",
    "\n",
    "위의 방법은 Lightning 인터페이스의 일부입니다.\n",
    "\n",
    "- training_step\n",
    "- validation_step\n",
    "- test_step\n",
    "- predict_step\n",
    "- configure_optimizers\n",
    "\n",
    "다음 예제에는 train 루프와 val 루프는 정확히 동일합니다. 물론 이 코드를 재사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daf79b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(28 * 28, 256), nn.ReLU(), nn.Linear(256, latent_dim))\n",
    "        self.decoder = nn.Sequential(nn.Linear(latent_dim, 256), nn.ReLU(), nn.Linear(256, 28 * 28))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.shared_step(batch)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def shared_step(self, batch):\n",
    "        x, _ = batch\n",
    "\n",
    "        # encode\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        # decode\n",
    "        recons = self.decoder(z)\n",
    "\n",
    "        # loss\n",
    "        return nn.functional.mse_loss(recons, x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350ff3f5",
   "metadata": {},
   "source": [
    "우리는 모든 루프에서 사용할 수 있는 `shared_step`이라는 새 메서드를 만듭니다. 이 메소드 이름은 임의적이며 예약되지 않습니다.\n",
    "\n",
    "### Inference in Research\n",
    "\n",
    "시스템에서 추론을 수행하려는 경우 LightningModule에 forward 메서드를 추가할 수 있습니다.\n",
    "\n",
    "> <span style='color:blue'>NOTE:</span> forward를 사용할 때 `eval()`을 호출하고 `no_grad()` context manager를 사용해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f660ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "model = Autoencoder()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    reconstruction = model(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b86ea",
   "metadata": {},
   "source": [
    "`forward`를 추가하는 것의 장점은 복잡한 시스템에서 텍스트 생성과 같은 훨씬 더 복잡한 추론 절차를 수행할 수 있다는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(pl.LightningModule):\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self(x)\n",
    "        hidden_states = self.encoder(embeddings)\n",
    "        for h in hidden_states:\n",
    "            # decode\n",
    "            ...\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ff6e99",
   "metadata": {},
   "source": [
    "추론을 확장하려는 경우에는 [`predict_step()`](https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule.predict_step)을 사용해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d61f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx = None)\n",
    "        # this calls forward\n",
    "        return self(batch)\n",
    "\n",
    "data_module = ...\n",
    "model = Autoencoder()\n",
    "trainer = Trainer(gpus=2)\n",
    "trainer.predict(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aefb35",
   "metadata": {},
   "source": [
    "### Inference in Production\n",
    "\n",
    "프로덕션과 같은 경우 LightningModule 내에서 다른 모델을 반복할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109dd096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics import functional as FM\n",
    "\n",
    "class ClassificationTask(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "        metrics = {'val_acc': acc, 'val_loss': loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "        metrics = {'test_acc': acc, 'test_loss': loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "\n",
    "    def _shared_eval_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = FM.accuracy(y_hat, y)\n",
    "        return loss, acc\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e7b434",
   "metadata": {},
   "source": [
    "그런 다음 이 작업에 적합하도록 임의의 모델을 전달합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b59847",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [resnet50(), vgg16(), BidirectionalRNN()]:\n",
    "    task = ClassificationTask(model)\n",
    "\n",
    "    trainer = Trainer(gpus=2)\n",
    "    trainer.fit(task, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed412dd",
   "metadata": {},
   "source": [
    "작업들은 GAN 학습, Self-supervised 또는 RL 구현과 같이 복잡한 것도 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1136a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANTask(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super().__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53221013",
   "metadata": {},
   "source": [
    "다음 상황처럼 사용하면 모델을 작업에서 분리할 수 있으므로 LightningModule에 보관할 필요 없이 프로덕션에서 사용할 수 있습니다.\n",
    "\n",
    "- onnx로 내보낼 수 있습니다.\n",
    "- 또는 Jit을 사용하여 추적합니다.\n",
    "- 또는 파이썬 런타임에서 실행하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7897de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = ClassificationTask(model)\n",
    "\n",
    "trainer = Trainer(gpus=2)\n",
    "trainer.fit(task, train_dataloader, val_dataloader)\n",
    "\n",
    "# use model after training or load weights and drop into the production system\n",
    "model.eval()\n",
    "y_hat = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99635d62",
   "metadata": {},
   "source": [
    "LightningModule API에 대한 더 자세한 내용은 [여기](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#lightningmodule-api)를 참고하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d16db0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
